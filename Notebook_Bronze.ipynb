{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8dd1828-1b35-4d55-b732-ea2a5efec893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìò Documenta√ß√£o da Camada Bronze ‚Äì Notebook de Ingest√£o\n",
    "\n",
    "Este notebook realiza a **extra√ß√£o de dados do banco PostgreSQL** hospedado no Azure e grava as tabelas na **camada Bronze** do Data Lake em formato Delta. Abaixo est√£o as a√ß√µes executadas:\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Configura√ß√£o da Conex√£o\n",
    "- Defini√ß√£o da **URL JDBC** para conex√£o com o banco PostgreSQL (`jdbc:postgresql://.../vendas?sslmode=require`).  \n",
    "- Cria√ß√£o de um dicion√°rio `connection_properties` contendo:\n",
    "  - Usu√°rio (`user`)  \n",
    "  - Senha (`password`)  \n",
    "  - Driver (`org.postgresql.Driver`)  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Extra√ß√£o de Dados (Leitura via Spark)\n",
    "Foram extra√≠das as seguintes tabelas do schema `public` no PostgreSQL:\n",
    "\n",
    "- **Clientes** ‚Üí `df_clientes`  \n",
    "- **Produtos** ‚Üí `df_produtos`  \n",
    "- **Pedidos** ‚Üí `df_pedido`  \n",
    "- **Itens_Pedido** ‚Üí `df_itens_pedido`  \n",
    "\n",
    "Cada DataFrame foi exibido com `display()` para valida√ß√£o.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Persist√™ncia na Camada Bronze\n",
    "Os DataFrames foram gravados em formato **Delta** na camada Bronze, utilizando `.write.format(\"delta\").mode(\"append\").saveAsTable(...)`.\n",
    "\n",
    "- `df_clientes` ‚Üí `bronze.vendas_clientes`  \n",
    "- `df_produtos` ‚Üí `bronze.vendas_produtos`  \n",
    "- `df_pedido` ‚Üí `bronze.vendas_pedidos`  \n",
    "- `df_itens_pedido` ‚Üí `bronze.vendas_itens__pedidos`  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo da Camada Bronze\n",
    "- Armazenar os dados **brutos** vindos do sistema transacional (PostgreSQL).  \n",
    "- Garantir **persist√™ncia confi√°vel** em formato Delta para futuras transforma√ß√µes.  \n",
    "- Servir como base para a constru√ß√£o das camadas **Silver** e **Gold** dentro da arquitetura **Medallion**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc43c4a-4e8d-4fe2-8f2b-090089ec11e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = (\n",
    "    \"jdbc:postgresql://azuremaniezzijp.postgres.database.azure.com:5432/vendas\"\n",
    "    \"?sslmode=require\"\n",
    ")\n",
    "\n",
    "# OBSERVA√á√ÉO : NA URL SEMPRE LEMBRE DE COLOCAR O BANCO DE DADOS NO FINAL DA URL\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": \"PgAdminJp\",\n",
    "    \"password\": \"Morango99*\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a49163-2235-40c1-8d79-899b00038f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extraindo tabela Clientes do banco postgres\n",
    "\n",
    "\n",
    "df_clientes = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.clientes\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "display(df_clientes)\n",
    "\n",
    "\n",
    "# Extraindo tabela Produtos do banco postgres\n",
    "\n",
    "df_produtos = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.produtos\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "display(df_produtos)\n",
    "\n",
    "\n",
    "\n",
    "# Extraindo tabela Pedidos do banco postgres\n",
    "\n",
    "df_pedido = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.pedido\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "\n",
    "display(df_pedido)\n",
    "\n",
    "# Extraindo tabela Itens_Pedidos do banco postgres\n",
    "\n",
    "df_itens_pedido = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.itens_pedido\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "display(df_itens_pedido)\n",
    "\n",
    "df_clientes.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.vendas_clientes\")\n",
    "\n",
    "\n",
    "df_produtos.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.vendas_produtos\")\n",
    "\n",
    "\n",
    "df_pedido.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.vendas_pedidos\")\n",
    "\n",
    "\n",
    "df_itens_pedido.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.vendas_itens__pedidos\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc915b04-6c64-4558-88b2-9e62f0a5d468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Auditoria de Cargas na Camada Bronze\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa um mecanismo de **auditoria** para rastrear cargas de dados na camada Bronze da arquitetura Medallion.\n",
    "\n",
    "## ‚öôÔ∏è Funcionamento\n",
    "- Ap√≥s cada ingest√£o de tabela (Clientes, Produtos, Pedidos, Itens_Pedido), o notebook cria registros de auditoria.  \n",
    "- Cada registro cont√©m:\n",
    "  - **tabela**: nome da tabela carregada.  \n",
    "  - **ingestion_time**: data e hora da carga.  \n",
    "  - **destino**: nome da tabela Bronze onde os dados foram gravados.  \n",
    "\n",
    "## Implementa√ß√£o\n",
    "- Usa `datetime.now()` para capturar o hor√°rio da ingest√£o.  \n",
    "- Cria uma lista de `Row` com os metadados de cada carga.  \n",
    "- Converte em um DataFrame Spark (`audit_df`).  \n",
    "- Grava os registros na tabela **`bronze.audit_log`** em formato Delta, com `.mode(\"append\")` para manter o hist√≥rico.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991aa39f-f257-45d7-a5f9-8ed04b7d4aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Cria log de auditoria\n",
    "log_data = [\n",
    "    Row(\n",
    "        tabela=\"clientes\",\n",
    "        ingestion_time=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        destino=\"bronze.vendas_clientes\"\n",
    "    ),\n",
    "    Row(\n",
    "        tabela=\"produtos\",\n",
    "        ingestion_time=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        destino=\"bronze.vendas_produtos\"\n",
    "    ),\n",
    "    Row(\n",
    "        tabela=\"pedido\",\n",
    "        ingestion_time=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        destino=\"bronze.vendas_pedidos\"\n",
    "    ),\n",
    "    Row(\n",
    "        tabela=\"itens_pedido\",\n",
    "        ingestion_time=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        destino=\"bronze.vendas_itens__pedidos\"\n",
    "    )\n",
    "]\n",
    "\n",
    "audit_df = spark.createDataFrame(log_data)\n",
    "\n",
    "audit_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze.audit_log\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
