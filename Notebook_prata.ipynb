{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a383a1-9fff-4c63-8bac-815837d3def0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Catálogo Clientes\n",
    "\n",
    "## 1. Coluna `nome`\n",
    "- Remoção de caracteres invisíveis (`\\u00A0`, `\\u2000-\\u200B`).\n",
    "- Aplicação de `trim` para retirar espaços extras.\n",
    "- Conversão para formato capitalizado com `initcap`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Coluna `cidade`\n",
    "- Remoção de caracteres invisíveis.\n",
    "- Eliminação de prefixos como **da**, **de**, **do**, **das** no início do texto.\n",
    "- Aplicação de `trim` e `initcap` para padronização.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Coluna `email`\n",
    "- Remoção de espaços extras com `trim`.\n",
    "- Conversão para letras maiúsculas com `upper`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tratamento de Valores Nulos\n",
    "- Substituição de valores `NULL`:\n",
    "  - `email` → `\"Desconhecido\"`\n",
    "  - `nome` → `\"Desconhecido\"`\n",
    "  - `cidade` → `\"Não Informado\"`\n",
    "- Remoção de linhas sem `id` (`na.drop(subset=[\"id\"])`).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ed2c41-b93d-4b00-969a-0700429f78b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forma correta para ler uma tabela Delta registrada no catálogo/schema\n",
    "df1 = spark.table(\"hive_metastore.bronze.vendas_clientes\")\n",
    "display(df1)\n",
    "\n",
    "\n",
    "# df.filter(col(\"nome\").isNull()).show();\n",
    "\n",
    "# #Filtra registros sem valores nulos \n",
    "# df.filter(col(\"nome\").isNotNull()).show();\n",
    "\n",
    "# #Conta quantos valores nulos existem em uma coluna \n",
    "# df.filter(col(\"nome\").isNull()).count();\n",
    "\n",
    "# # Filtra valrias colunas nulas de uma vez \n",
    "# df.filter(col(\"nome\").isNull() | col(\"cidade\").isNull()).show();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de53ff33-884c-41d8-b342-c840cf79f092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, initcap, regexp_replace, upper\n",
    "\n",
    "df1 = (\n",
    "    df1.withColumn(\n",
    "        \"nome\",\n",
    "        initcap(\n",
    "            trim(\n",
    "                regexp_replace(col(\"nome\"), r\"[\\u00A0\\u2000-\\u200B]\", \" \")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cidade\",\n",
    "        initcap(\n",
    "            trim(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(col(\"cidade\"), r\"[\\u00A0\\u2000-\\u200B]\", \" \"), \n",
    "                    r\"^(da|de|do|das)\\s+\", \"\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"email\", upper(trim(col(\"email\"))))\n",
    "    .fillna({\"email\": \"Desconhecido\", \"nome\": \"Desconhecido\", \"cidade\": \"Não Informado\"})\n",
    "    .na.drop(subset=[\"id\"])\n",
    "    .dropDuplicates([\"id\"])   \n",
    ")\n",
    "\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c71de77-e014-4893-aecc-a7d8a88325c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE DATABASE IF NOT EXISTS silver;\n",
    "\n",
    "# CREATE TABLE IF NOT EXISTS silver.vendas_clientes (\n",
    "#   id INT,\n",
    "#   nome STRING,\n",
    "#   email STRING,\n",
    "#   cidade STRING\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e725e2d3-9224-48c7-af96-eb5a3d18b844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_clientes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b7943a-b338-4273-b1c5-f09db56a8e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Catálogo Produtos\n",
    "\n",
    "## 1. Leitura da Tabela Bronze\n",
    "- Carregamento da tabela **`hive_metastore.bronze.vendas_produtos`** como fonte inicial.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Separação da Coluna `nome`\n",
    "- Divisão da coluna `nome` em duas partes:\n",
    "  - **`categoria`**: primeira palavra.\n",
    "  - **`numero`**: segunda palavra, convertida para inteiro.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Seleção de Colunas\n",
    "- Mantidas apenas as colunas relevantes: `id`, `nome`, `categoria`, `numero`, `preco`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Correção de Categorias\n",
    "- Criada a coluna **`categoria_corrigida`** com valores padronizados.\n",
    "- Correção de erros de digitação (ex.: \"movel\" → \"Móvel\", \"eletronico\" → \"Eletrônico\").\n",
    "- Tratamento de valores nulos: substituídos por \"Desconhecido\".\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Criação de `nome_corrigido`\n",
    "- Construída a coluna **`nome_corrigido`** concatenando `categoria_corrigida` e `numero`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Limpeza de Dados\n",
    "- Removidos registros com `id` nulo.\n",
    "- Removidos espaços extras em `nome_corrigido` e `categoria_corrigida`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Escrita na Tabela Silver\n",
    "- Persistência dos dados transformados na tabela **`silver.vendas_produtos`**.\n",
    "- Formato: **Delta**.\n",
    "- Modo: **append** (acréscimo de registros).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae0fd50-55fa-4979-bcca-f9463cb6decd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.table(\"hive_metastore.bronze.vendas_produtos\")\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351820f2-62e4-43cb-9d1b-e88c899ae880",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769634881495}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# separa em duas partes: antes e depois do espaço\n",
    "df2 = (\n",
    "    df2.withColumn(\"categoria\", split(col(\"nome\"), \" \")[0])\n",
    "        .withColumn(\"numero\", split(col(\"nome\"), \" \")[1].cast(\"int\"))\n",
    "        .select(\"id\", \"nome\", \"categoria\", \"numero\", \"preco\")\n",
    "        .dropDuplicates([\"id\"])  \n",
    ")\n",
    "\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18063b0-5a03-473b-a91d-6dfd3206c416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "correcoes = {\n",
    "    \n",
    "    \"movel\": \"Móvel\",\n",
    "    \"móvei\": \"Móvel\",  \n",
    "    \"eletronico\": \"Eletrônico\",\n",
    "    \"brinquedo\": \"Brinquedo\",\n",
    "    \"alimento\": \"Alimento\",\n",
    "    \"roupa\": \"Roupa\",\n",
    "    \"livro\": \"Livro\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "def corrigir_categoria(cat):\n",
    "    if cat is None:\n",
    "        return \"Desconhecido\"\n",
    "    return correcoes.get(cat.lower(), cat)\n",
    "\n",
    "corrigir_udf = udf(corrigir_categoria, StringType())\n",
    "\n",
    "df2 = df2.withColumn(\"categoria_corrigida\", corrigir_udf(col(\"categoria\")))\n",
    "\n",
    "df2 = df2.select(\"id\",\"nome\",\"categoria_corrigida\", \"numero\", \"preco\")\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3314d2b4-f52b-412c-b052-0aad1e891f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df2 = df2.withColumn(\n",
    "    \"nome_corrigido\",\n",
    "    concat_ws(\" \", col(\"categoria_corrigida\"), col(\"numero\"))\n",
    ")\n",
    "\n",
    "df2 = df2.select(\"id\", \"nome_corrigido\",\"categoria_corrigida\")\n",
    "\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28b911f-79c3-4781-aac2-485904c18661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col,trim, abs , round\n",
    "\n",
    "df2 = df2.select(\"id\", \"nome_corrigido\",\"categoria_corrigida\")\n",
    "\n",
    "df2 = df2.na.drop(subset=[\"id\"])\n",
    "\n",
    "df2 = df2.withColumn(\"nome_corrigido\", trim(col(\"nome_corrigido\"))) \\\n",
    "         .withColumn(\"categoria_corrigida\", trim(col(\"categoria_corrigida\")))\n",
    "\n",
    "display(df2)\n",
    "\n",
    "df2.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_produtos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea49ebb-c242-422b-8aeb-3f8df5cf4600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Catálogo itens pedidos\n",
    "\n",
    "## 1. Leitura da Tabela Bronze\n",
    "- Carregamento da tabela **`hive_metastore.bronze.vendas_itens__pedidos`** como fonte inicial.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Remoção de Registros Inválidos\n",
    "- Exclusão de linhas com **`id` nulo** para garantir integridade dos dados.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Correção da Coluna `preco_unitario`\n",
    "- Criada a coluna **`preco_unitario_corrigido`**:\n",
    "  - Valores nulos substituídos por **0**.\n",
    "  - Valores negativos transformados em positivos com `abs`.\n",
    "  - Arredondamento aplicado para **2 casas decimais**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Correção da Coluna `quantidade`\n",
    "- Criada a coluna **`quantidade_corrigida`**:\n",
    "  - Valores nulos substituídos por **0**.\n",
    "  - Valores válidos mantidos sem alteração.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Seleção de Colunas Relevantes\n",
    "- Mantidas apenas as colunas necessárias para análise:\n",
    "  - `id`\n",
    "  - `pedido_id`\n",
    "  - `produto_id`\n",
    "  - `quantidade_corrigida`\n",
    "  - `preco_unitario_corrigido`\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Escrita na Tabela Silver\n",
    "- Persistência dos dados transformados na tabela **`silver.vendas_itens__pedidos`**.\n",
    "- Formato: **Delta**.\n",
    "- Modo: **append** (acréscimo de registros).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac4a9e5-0f5b-464b-8a8b-f794ace3685c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, abs, round\n",
    "\n",
    "df3 = spark.table(\"hive_metastore.bronze.vendas_itens__pedidos\")\n",
    "\n",
    "df3 = df3.na.drop(subset=[\"id\"])\n",
    "\n",
    "df3 = (\n",
    "    df3\n",
    "    .withColumn(\n",
    "        \"preco_unitario_corrigido\",\n",
    "        when(col(\"preco_unitario\").isNull(), 0)\n",
    "        .otherwise(abs(col(\"preco_unitario\")))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"quantidade_corrigida\",\n",
    "        when(col(\"quantidade\").isNull(), 0)\n",
    "        .otherwise(col(\"quantidade\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df3 = df3.withColumn(\"preco_unitario_corrigido\", round(col(\"preco_unitario_corrigido\"), 2))\n",
    "\n",
    "df3 = (\n",
    "    df3.select(\"id\", \"pedido_id\", \"produto_id\", \"quantidade_corrigida\", \"preco_unitario_corrigido\")\n",
    "        .dropDuplicates([\"id\"])   \n",
    ")\n",
    "\n",
    "display(df3)\n",
    "df3.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_itens__pedidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95c3d6b-2428-449c-9b5e-4a1275fcc38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#catálogo pedidos\n",
    "\n",
    "\n",
    "\n",
    "## 1. Leitura da Tabela Bronze\n",
    "- Carregamento da tabela **`hive_metastore.bronze.vendas_pedidos`** como fonte inicial.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Seleção de Colunas\n",
    "- Mantidas apenas as colunas relevantes para análise:\n",
    "  - `id`\n",
    "  - `cliente_id`\n",
    "  - `data`\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Remoção de Registros Inválidos\n",
    "- Exclusão de linhas com **`id` nulo** para garantir integridade dos dados.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Escrita na Tabela Silver\n",
    "- Persistência dos dados transformados na tabela **`silver.vendas_pedidos`**.\n",
    "- Modo: **append** (acréscimo de registros).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4896dd-43d2-49b2-b743-a92d64501463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4 = spark.table(\"hive_metastore.bronze.vendas_pedidos\")\n",
    "\n",
    "df4 = df4.select(\"id\", \"cliente_id\", \"data\")\n",
    "\n",
    "df4 = df4.na.drop(subset=[\"id\"])\n",
    "\n",
    "df4 = df4.dropDuplicates([\"id\"])\n",
    "\n",
    "display(df4)\n",
    "\n",
    "df4.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_pedidos\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_prata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
