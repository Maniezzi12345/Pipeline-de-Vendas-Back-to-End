{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a383a1-9fff-4c63-8bac-815837d3def0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Cat√°logo Clientes\n",
    "\n",
    "##  Transforma√ß√µes aplicadas no DataFrame `df_clientes`\n",
    "- **Coluna `nome`**\n",
    "  - Remove caracteres invis√≠veis (`\\u00A0`, `\\u2000-\\u200B`).\n",
    "  - Aplica `trim` para retirar espa√ßos extras.\n",
    "  - Converte para formato capitalizado (`initcap`).\n",
    "\n",
    "- **Coluna `cidade`**\n",
    "  - Remove caracteres invis√≠veis.\n",
    "  - Elimina prefixos como `da`, `de`, `do`, `das` no in√≠cio.\n",
    "  - Aplica `trim` e `initcap` para padronizar.\n",
    "\n",
    "- **Coluna `email`**\n",
    "  - Remove espa√ßos extras (`trim`).\n",
    "  - Converte para mai√∫sculas (`upper`).\n",
    "\n",
    "- **Tratamento de valores nulos**\n",
    "  - Substitui `NULL` em:\n",
    "    - `email` ‚Üí `\"Desconhecido\"`\n",
    "    - `nome` ‚Üí `\"Desconhecido\"`\n",
    "    - `cidade` ‚Üí `\"N√£o Informado\"`\n",
    "  - Remove linhas sem `id` (`na.drop(subset=[\"id\"])`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ed2c41-b93d-4b00-969a-0700429f78b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forma correta para ler uma tabela Delta registrada no cat√°logo/schema\n",
    "df1 = spark.table(\"hive_metastore.bronze.vendas_clientes\")\n",
    "display(df1)\n",
    "\n",
    "\n",
    "# df.filter(col(\"nome\").isNull()).show();\n",
    "\n",
    "# #Filtra registros sem valores nulos \n",
    "# df.filter(col(\"nome\").isNotNull()).show();\n",
    "\n",
    "# #Conta quantos valores nulos existem em uma coluna \n",
    "# df.filter(col(\"nome\").isNull()).count();\n",
    "\n",
    "# # Filtra valrias colunas nulas de uma vez \n",
    "# df.filter(col(\"nome\").isNull() | col(\"cidade\").isNull()).show();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de53ff33-884c-41d8-b342-c840cf79f092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, initcap, regexp_replace, upper\n",
    "\n",
    "# Fun√ß√£o para limpar espa√ßos invis√≠veis\n",
    "def limpar_espacos(campo):\n",
    "    return regexp_replace(col(campo), r\"[\\u00A0\\u2000-\\u200B]\", \" \")\n",
    "\n",
    "# Fun√ß√£o para normalizar nomes pr√≥prios\n",
    "def normalizar_nome(campo):\n",
    "    return initcap(trim(limpar_espacos(campo)))\n",
    "\n",
    "# Fun√ß√£o para normalizar cidades (removendo prefixos da/de/do/das)\n",
    "def normalizar_cidade(campo):\n",
    "    return initcap(\n",
    "        trim(\n",
    "            regexp_replace(\n",
    "                limpar_espacos(campo),\n",
    "                r\"^(da|de|do|das)\\s+\", \"\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_clientes = (\n",
    "    df1.withColumn(\"nome\", normalizar_nome(\"nome\"))\n",
    "      .withColumn(\"cidade\", normalizar_cidade(\"cidade\"))\n",
    "      .withColumn(\"email\", upper(trim(col(\"email\"))))\n",
    "      .na.fill({\n",
    "          \"email\": \"DESCONHECIDO\",\n",
    "          \"nome\": \"DESCONHECIDO\",\n",
    "          \"cidade\": \"N√ÉO INFORMADO\"\n",
    "      })\n",
    "      .na.drop(subset=[\"id\"])\n",
    ")\n",
    "\n",
    "display(df_clientes)\n",
    "\n",
    "\n",
    "\n",
    "# df_clientes.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .saveAsTable(\"silver.vendas_clientes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b7943a-b338-4273-b1c5-f09db56a8e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì¶ Tabela `vendas_produtos`\n",
    "\n",
    "### Tratativas Aplicadas\n",
    "1. **Separa√ß√£o de nome em categoria e n√∫mero**\n",
    "   - `categoria`: primeira parte do campo `nome`.\n",
    "   - `numero`: segunda parte do campo `nome`, convertido para inteiro.\n",
    "\n",
    "2. **Corre√ß√£o de categorias**\n",
    "   - Uso de dicion√°rio de corre√ß√µes para padronizar valores.\n",
    "   - Exemplos: `\"movel\"` ‚Üí `\"M√≥vel\"`, `\"eletronico\"` ‚Üí `\"Eletr√¥nico\"`.\n",
    "   - Se nulo ‚Üí `\"Desconhecido\"`.\n",
    "\n",
    "3. **Reconstru√ß√£o do nome corrigido**\n",
    "   - `nome_corrigido`: concatena√ß√£o de `categoria_corrigida` + `numero`.\n",
    "\n",
    "4. **Limpeza de dados**\n",
    "   - Remo√ß√£o de registros sem `id`.\n",
    "   - Aplica√ß√£o de `trim` para remover espa√ßos extras.\n",
    "\n",
    "5. **Sele√ß√£o final de colunas**\n",
    "   - Mantidas: `id`, `nome_corrigido`, `categoria_corrigida`.\n",
    "\n",
    "6. **Persist√™ncia**\n",
    "   - Dados gravados em `silver.vendas_produtos`.\n",
    "   - Formato: Delta Lake.\n",
    "   - Modo: `append`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae0fd50-55fa-4979-bcca-f9463cb6decd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.functions import when, col,trim, abs , round\n",
    "\n",
    "\n",
    "df2 = spark.table(\"hive_metastore.bronze.vendas_produtos\")\n",
    "display(df2)\n",
    "\n",
    "# separa em duas partes: antes e depois do espa√ßo\n",
    "df2 = df2.withColumn(\"categoria\", split(col(\"nome\"), \" \")[0]) \\\n",
    "         .withColumn(\"numero\", split(col(\"nome\"), \" \")[1].cast(\"int\"))\n",
    "\n",
    "df2 = df2.select(\"id\",\"nome\",\"categoria\", \"numero\", \"preco\")\n",
    "\n",
    "df2.select('categoria').distinct().show()\n",
    "\n",
    "\n",
    "correcoes = {\n",
    "    \n",
    "    \"movel\": \"M√≥vel\",\n",
    "    \"m√≥vei\": \"M√≥vel\",   # corrige o erro \"M√≥vei\"\n",
    "    \"eletronico\": \"Eletr√¥nico\",\n",
    "    \"brinquedo\": \"Brinquedo\",\n",
    "    \"alimento\": \"Alimento\",\n",
    "    \"roupa\": \"Roupa\",\n",
    "    \"livro\": \"Livro\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "def corrigir_categoria(cat):\n",
    "    if cat is None:\n",
    "        return \"Desconhecido\"\n",
    "    return correcoes.get(cat.lower(), cat)\n",
    "\n",
    "corrigir_udf = udf(corrigir_categoria, StringType())\n",
    "\n",
    "df2 = df2.withColumn(\"categoria_corrigida\", corrigir_udf(col(\"categoria\")))\n",
    "\n",
    "df2 = df2.select(\"id\",\"nome\",\"categoria_corrigida\", \"numero\", \"preco\")\n",
    "\n",
    "\n",
    "df2 = df2.withColumn(\n",
    "    \"nome_corrigido\",\n",
    "    concat_ws(\" \", col(\"categoria_corrigida\"), col(\"numero\"))\n",
    ")\n",
    "\n",
    "df2 = df2.select(\"id\", \"nome_corrigido\",\"categoria_corrigida\")\n",
    "\n",
    "\n",
    "\n",
    "df2 = df2.select(\"id\", \"nome_corrigido\",\"categoria_corrigida\")\n",
    "\n",
    "df2 = df2.na.drop(subset=[\"id\"])\n",
    "\n",
    "df2 = df2.withColumn(\"nome_corrigido\", trim(col(\"nome_corrigido\"))) \\\n",
    "         .withColumn(\"categoria_corrigida\", trim(col(\"categoria_corrigida\")))\n",
    "\n",
    "display(df2)\n",
    "\n",
    "df2.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_produtos\")\n",
    "\n",
    "\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea49ebb-c242-422b-8aeb-3f8df5cf4600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cat√°logo de vendas_itens__pedidos\n",
    "\n",
    "Este documento descreve as regras de tratamento aplicadas √† tabela `vendas_itens__pedidos` no processo de ETL (Extract, Transform, Load) utilizando **PySpark**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo\n",
    "- Garantir integridade dos dados.\n",
    "- Corrigir valores inconsistentes (nulos ou negativos).\n",
    "- Padronizar colunas para an√°lise.\n",
    "- Persistir os dados limpos na camada **Silver**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Tratativas Aplicadas\n",
    "\n",
    "### 1. Remo√ß√£o de Registros Inv√°lidos\n",
    "- Linhas com `id` nulo s√£o descartadas.\n",
    "- Isso garante que cada item de pedido tenha uma chave prim√°ria v√°lida.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Corre√ß√£o de Pre√ßo Unit√°rio\n",
    "- Coluna: `preco_unitario_corrigido`\n",
    "- Regras:\n",
    "  - Se `preco_unitario` for **nulo**, substitui por `0`.\n",
    "  - Se `preco_unitario` for **negativo**, aplica valor absoluto.\n",
    "  - Arredondamento para **2 casas decimais**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Corre√ß√£o de Quantidade\n",
    "- Coluna: `quantidade_corrigida`\n",
    "- Regras:\n",
    "  - Se `quantidade` for **nula**, substitui por `0`.\n",
    "  - Caso contr√°rio, mant√©m o valor original.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Sele√ß√£o de Colunas Relevantes\n",
    "Somente as seguintes colunas s√£o mantidas:\n",
    "- `id`\n",
    "- `pedido_id`\n",
    "- `produto_id`\n",
    "- `quantidade_corrigida`\n",
    "- `preco_unitario_corrigido`\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Persist√™ncia dos Dados\n",
    "- Os dados tratados s√£o gravados na tabela **Silver**:\n",
    "  - Nome: `silver.vendas_itens__pedidos`\n",
    "  - Formato: **Delta Lake**\n",
    "  - Modo: `append` (adiciona registros sem sobrescrever os existentes)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac4a9e5-0f5b-464b-8a8b-f794ace3685c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, abs\n",
    "\n",
    "df3 = spark.table(\"hive_metastore.bronze.vendas_itens__pedidos\")\n",
    "\n",
    "\n",
    "df3 = df3.na.drop(subset=[\"id\"])\n",
    "\n",
    "\n",
    "df3 = (\n",
    "    df3\n",
    "    .withColumn(\n",
    "        \"preco_unitario_corrigido\",\n",
    "        when(col(\"preco_unitario\").isNull(), 0)\n",
    "        .otherwise(abs(col(\"preco_unitario\")))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"quantidade_corrigida\",\n",
    "        when(col(\"quantidade\").isNull(), 0)\n",
    "        .otherwise(col(\"quantidade\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df3 = df3.withColumn(\"preco_unitario_corrigido\", round(col(\"preco_unitario_corrigido\"), 2))\n",
    "\n",
    "df3 = df3.select(\"id\", \"pedido_id\",\"produto_id\",\"quantidade_corrigida\",\"preco_unitario_corrigido\")\n",
    "\n",
    "display(df3)\n",
    "\n",
    "\n",
    "df3.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_itens__pedidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95c3d6b-2428-449c-9b5e-4a1275fcc38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cat√°logo pedidos\n",
    "\n",
    "# Altera√ß√µes realizadas no DataFrame `df3`\n",
    "\n",
    "## 1. Leitura da Tabela Bronze\n",
    "- Carregamento da tabela **`hive_metastore.bronze.vendas_itens__pedidos`** como fonte inicial.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Remo√ß√£o de Registros Inv√°lidos\n",
    "- Exclus√£o de linhas com `id` nulo utilizando `na.drop`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Corre√ß√£o da Coluna `preco_unitario`\n",
    "- Criada a coluna **`preco_unitario_corrigido`**:\n",
    "  - Valores nulos substitu√≠dos por **0**.\n",
    "  - Valores negativos transformados em positivos com `abs`.\n",
    "  - Arredondamento aplicado para **2 casas decimais**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Corre√ß√£o da Coluna `quantidade`\n",
    "- Criada a coluna **`quantidade_corrigida`**:\n",
    "  - Valores nulos substitu√≠dos por **0**.\n",
    "  - Valores v√°lidos mantidos.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Sele√ß√£o de Colunas Relevantes\n",
    "- Mantidas apenas as colunas necess√°rias:\n",
    "  - `id`\n",
    "  - `pedido_id`\n",
    "  - `produto_id`\n",
    "  - `quantidade_corrigida`\n",
    "  - `preco_unitario_corrigido`\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Escrita na Tabela Silver\n",
    "- Persist√™ncia dos dados transformados na tabela **`silver.vendas_itens__pedidos`**.\n",
    "- Formato: **Delta**.\n",
    "- Modo: **append** (acr√©scimo de registros).\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4896dd-43d2-49b2-b743-a92d64501463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4 = spark.table(\"hive_metastore.bronze.vendas_pedidos\")\n",
    "\n",
    "df4 = df4.select(\"id\", \"cliente_id\", \"data\")\n",
    "\n",
    "df4 = df4.na.drop(subset=[\"id\"])\n",
    "\n",
    "df4 = df4.dropDuplicates([\"id\"])\n",
    "\n",
    "display(df4)\n",
    "\n",
    "df4.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.vendas_pedidos\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_prata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
